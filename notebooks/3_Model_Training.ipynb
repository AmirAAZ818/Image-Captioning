{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Captioning - Model Training\n",
    "\n",
    "In this notebook, we will train the image captioning model using the pre-extracted features from the CNN encoder. We will:\n",
    "\n",
    "1. Set up the data loaders for training and validation\n",
    "2. Build the caption model (combining the encoder and decoder)\n",
    "3. Define the training pipeline with teacher forcing\n",
    "4. Train the model with appropriate hyperparameters\n",
    "5. Monitor the training progress and validation performance\n",
    "6. Save the trained model for later evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import h5py\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.append('..')\n",
    "\n",
    "# Import project modules\n",
    "from models.encoder import EncoderCNN\n",
    "from models.decoder import DecoderRNN\n",
    "from models.caption_model import CaptionModel\n",
    "from utils.vocabulary import Vocabulary\n",
    "from utils.dataset import get_data_loaders\n",
    "from utils.trainer import CaptionTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data settings:\n",
      "  data_dir: ../data/flickr8k\n",
      "  batch_size: 32\n",
      "  num_workers: 4\n",
      "\n",
      "Model settings:\n",
      "  encoder_model: resnet18\n",
      "  embed_size: 256\n",
      "  hidden_size: 512\n",
      "  num_layers: 1\n",
      "  dropout: 0.5\n",
      "  decoder_type: lstm\n",
      "\n",
      "Training settings:\n",
      "  learning_rate: 0.0003\n",
      "  num_epochs: 2\n",
      "  early_stopping_patience: 5\n",
      "  save_dir: ../models\n",
      "\n",
      "Device settings:\n",
      "  device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Define configuration settings\n",
    "config = {\n",
    "    # Data settings\n",
    "    'data_dir': '../data/flickr8k',\n",
    "    'batch_size': 32,\n",
    "    'num_workers': 4,\n",
    "    \n",
    "    # Model settings\n",
    "    'encoder_model': 'resnet18',  # Options: 'resnet18', 'resnet50', 'mobilenet_v2'\n",
    "    'embed_size': 256,\n",
    "    'hidden_size': 512,\n",
    "    'num_layers': 1,\n",
    "    'dropout': 0.5,\n",
    "    'decoder_type': 'lstm',  # Options: 'lstm', 'gru'\n",
    "    \n",
    "    # Training settings\n",
    "    'learning_rate': 3e-4,\n",
    "    'num_epochs': 2,  # Increase for better results\n",
    "    'early_stopping_patience': 5,\n",
    "    'save_dir': '../models',\n",
    "    \n",
    "    # Device settings\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "}\n",
    "\n",
    "# Create save directory if it doesn't exist\n",
    "os.makedirs(config['save_dir'], exist_ok=True)\n",
    "\n",
    "# Print configuration\n",
    "for section, items in {\n",
    "    'Data': ['data_dir', 'batch_size', 'num_workers'],\n",
    "    'Model': ['encoder_model', 'embed_size', 'hidden_size', 'num_layers', 'dropout', 'decoder_type'],\n",
    "    'Training': ['learning_rate', 'num_epochs', 'early_stopping_patience', 'save_dir'],\n",
    "    'Device': ['device']\n",
    "}.items():\n",
    "    print(f\"\\n{section} settings:\")\n",
    "    for item in items:\n",
    "        print(f\"  {item}: {config[item]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 2986\n",
      "Training batches: 937\n",
      "Validation batches: 157\n",
      "Test batches: 5000\n"
     ]
    }
   ],
   "source": [
    "# Get data loaders\n",
    "train_loader, val_loader, test_loader, vocab = get_data_loaders(\n",
    "    data_dir=config['data_dir'],\n",
    "    batch_size=config['batch_size'],\n",
    "    num_workers=config['num_workers']\n",
    ")\n",
    "\n",
    "# Update vocabulary size in config\n",
    "config['vocab_size'] = len(vocab)\n",
    "print(f\"Vocabulary size: {config['vocab_size']}\")\n",
    "\n",
    "# Display data loader information\n",
    "print(f\"Training batches: {len(train_loader)}\")\n",
    "print(f\"Validation batches: {len(val_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Aanaconda\\envs\\image_captioning\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "====================================================================================================\n",
       "Layer (type:depth-idx)                             Output Shape              Param #\n",
       "====================================================================================================\n",
       "CaptionModel                                       [1, 19, 2986]             --\n",
       "├─EncoderCNN: 1-1                                  [1, 256]                  --\n",
       "│    └─ResNet: 2-1                                 [1, 512]                  --\n",
       "│    │    └─Conv2d: 3-1                            [1, 64, 64, 64]           (9,408)\n",
       "│    │    └─BatchNorm2d: 3-2                       [1, 64, 64, 64]           (128)\n",
       "│    │    └─ReLU: 3-3                              [1, 64, 64, 64]           --\n",
       "│    │    └─MaxPool2d: 3-4                         [1, 64, 32, 32]           --\n",
       "│    │    └─Sequential: 3-5                        [1, 64, 32, 32]           (147,968)\n",
       "│    │    └─Sequential: 3-6                        [1, 128, 16, 16]          (525,568)\n",
       "│    │    └─Sequential: 3-7                        [1, 256, 8, 8]            (2,099,712)\n",
       "│    │    └─Sequential: 3-8                        [1, 512, 4, 4]            (8,393,728)\n",
       "│    │    └─AdaptiveAvgPool2d: 3-9                 [1, 512, 1, 1]            --\n",
       "│    │    └─Identity: 3-10                         [1, 512]                  --\n",
       "│    └─Sequential: 2-2                             [1, 256]                  --\n",
       "│    │    └─Linear: 3-11                           [1, 256]                  131,328\n",
       "│    │    └─BatchNorm1d: 3-12                      [1, 256]                  512\n",
       "│    │    └─ReLU: 3-13                             [1, 256]                  --\n",
       "│    │    └─Dropout: 3-14                          [1, 256]                  --\n",
       "├─DecoderRNN: 1-2                                  [1, 19, 2986]             --\n",
       "│    └─Embedding: 2-3                              [1, 19, 256]              764,416\n",
       "│    └─Linear: 2-4                                 [1, 512]                  131,584\n",
       "│    └─LSTM: 2-5                                   [1, 19, 512]              1,576,960\n",
       "│    └─Dropout: 2-6                                [1, 19, 512]              --\n",
       "│    └─Linear: 2-7                                 [1, 19, 2986]             1,531,818\n",
       "====================================================================================================\n",
       "Total params: 15,313,130\n",
       "Trainable params: 4,136,618\n",
       "Non-trainable params: 11,176,512\n",
       "Total mult-adds (Units.MEGABYTES): 624.71\n",
       "====================================================================================================\n",
       "Input size (MB): 0.20\n",
       "Forward/backward pass size (MB): 13.55\n",
       "Params size (MB): 61.25\n",
       "Estimated Total Size (MB): 75.00\n",
       "===================================================================================================="
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create caption model\n",
    "model = CaptionModel(\n",
    "    embed_size=config['embed_size'],\n",
    "    hidden_size=config['hidden_size'],\n",
    "    vocab_size=config['vocab_size'],\n",
    "    num_layers=config['num_layers'],\n",
    "    encoder_model=config['encoder_model'],\n",
    "    decoder_type=config['decoder_type'],\n",
    "    dropout=config['dropout'],\n",
    "    train_encoder=False  # Don't train the encoder (use pre-trained weights)\n",
    ")\n",
    "\n",
    "# Print model architecture\n",
    "# print(model)\n",
    "summary(model, input_size=[(1, 3, 128, 128), (1, 20)], dtypes=[torch.float, torch.long])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters: 4136618\n",
      "Encoder parameters: 131,840 (3.2%)\n",
      "Decoder parameters: 4,004,778 (96.8%)\n"
     ]
    }
   ],
   "source": [
    "# Calculate model size\n",
    "def count_parameters(model: nn.Module):\n",
    "    \"\"\"Count the number of trainable parameters in a model.\"\"\"\n",
    "    num_of_trainable_parameters = sum([param.numel() for param in model.parameters() if param.requires_grad])\n",
    "    return num_of_trainable_parameters\n",
    "\n",
    "# Calculate and print model size\n",
    "num_params = count_parameters(model)\n",
    "print(f\"Number of trainable parameters: {num_params}\")\n",
    "\n",
    "# Calculate encoder and decoder sizes separately\n",
    "encoder_params = count_parameters(model.encoder)\n",
    "decoder_params = count_parameters(model.decoder)\n",
    "print(f\"Encoder parameters: {encoder_params:,} ({encoder_params / num_params * 100:.1f}%)\")\n",
    "print(f\"Decoder parameters: {decoder_params:,} ({decoder_params / num_params * 100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trainer\n",
    "trainer = CaptionTrainer(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    vocab=vocab,\n",
    "    device=config['device'],\n",
    "    learning_rate=config['learning_rate'],\n",
    "    model_save_dir=config['save_dir']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for 2 epochs...\n",
      "Training on device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Aanaconda\\envs\\image_captioning\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnum_epochs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_patience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mearly_stopping_patience\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevaluate_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgenerate_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Generate captions and calculate BLEU every 5 epochs\u001b[39;49;00m\n\u001b[0;32m      7\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\University of Kerman\\Term 8\\Deep Learning\\HW\\03\\image-captioning\\notebooks\\..\\utils\\trainer.py:233\u001b[0m, in \u001b[0;36mCaptionTrainer.train\u001b[1;34m(self, epochs, early_stopping_patience, save_best_only, evaluate_every, generate_every)\u001b[0m\n\u001b[0;32m    230\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    232\u001b[0m \u001b[38;5;66;03m# Train for one epoch\u001b[39;00m\n\u001b[1;32m--> 233\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[38;5;66;03m# Validate\u001b[39;00m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m evaluate_every \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32me:\\University of Kerman\\Term 8\\Deep Learning\\HW\\03\\image-captioning\\notebooks\\..\\utils\\trainer.py:96\u001b[0m, in \u001b[0;36mCaptionTrainer.train_epoch\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     94\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m     95\u001b[0m batch_losses \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 96\u001b[0m progress_bar \u001b[38;5;241m=\u001b[39m tqdm(\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m, total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_loader), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_epoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     98\u001b[0m \u001b[38;5;66;03m# TODO: Implement the training loop for one epoch\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;66;03m# 1. Iterate through batches in the training data loader\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, (images, captions, image_id) \u001b[38;5;129;01min\u001b[39;00m progress_bar:\n\u001b[0;32m    101\u001b[0m \u001b[38;5;66;03m# 2. Move data (images and captions) to the device\u001b[39;00m\n",
      "File \u001b[1;32md:\\Aanaconda\\envs\\image_captioning\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:493\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    491\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[0;32m    492\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 493\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Aanaconda\\envs\\image_captioning\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:424\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    422\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    423\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_worker_number_rationality()\n\u001b[1;32m--> 424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Aanaconda\\envs\\image_captioning\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1171\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[1;34m(self, loader)\u001b[0m\n\u001b[0;32m   1164\u001b[0m w\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1165\u001b[0m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[0;32m   1166\u001b[0m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[0;32m   1167\u001b[0m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[0;32m   1168\u001b[0m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[0;32m   1169\u001b[0m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[0;32m   1170\u001b[0m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[1;32m-> 1171\u001b[0m \u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1172\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_queues\u001b[38;5;241m.\u001b[39mappend(index_queue)\n\u001b[0;32m   1173\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers\u001b[38;5;241m.\u001b[39mappend(w)\n",
      "File \u001b[1;32md:\\Aanaconda\\envs\\image_captioning\\Lib\\multiprocessing\\process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemon\u001b[39m\u001b[38;5;124m'\u001b[39m), \\\n\u001b[0;32m    119\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemonic processes are not allowed to have children\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    120\u001b[0m _cleanup()\n\u001b[1;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[1;32md:\\Aanaconda\\envs\\image_captioning\\Lib\\multiprocessing\\context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[1;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mProcess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Aanaconda\\envs\\image_captioning\\Lib\\multiprocessing\\context.py:336\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[0;32m    335\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopen_spawn_win32\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[1;32m--> 336\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Aanaconda\\envs\\image_captioning\\Lib\\multiprocessing\\popen_spawn_win32.py:95\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, process_obj)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     94\u001b[0m     reduction\u001b[38;5;241m.\u001b[39mdump(prep_data, to_child)\n\u001b[1;32m---> 95\u001b[0m     \u001b[43mreduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_child\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     97\u001b[0m     set_spawning_popen(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32md:\\Aanaconda\\envs\\image_captioning\\Lib\\multiprocessing\\reduction.py:60\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(obj, file, protocol)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdump\u001b[39m(obj, file, protocol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     59\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m     \u001b[43mForkingPickler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = trainer.train(\n",
    "    epochs=config['num_epochs'],\n",
    "    early_stopping_patience=config['early_stopping_patience'],\n",
    "    evaluate_every=1,\n",
    "    generate_every=1  # Generate captions and calculate BLEU every 5 epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analyze Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axs = trainer.plot_history()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best epoch\n",
    "best_epoch = np.argmin(trainer.history['val_loss'])\n",
    "print(f\"Best epoch: {best_epoch + 1}\")\n",
    "print(f\"Best validation loss: {trainer.history['val_loss'][best_epoch]:.4f}\")\n",
    "\n",
    "# If BLEU scores were calculated\n",
    "bleu_epochs = [i for i, bleu in enumerate(trainer.history['val_bleu']) if bleu > 0]\n",
    "if bleu_epochs:\n",
    "    best_bleu_epoch = bleu_epochs[np.argmax([trainer.history['val_bleu'][i] for i in bleu_epochs])]\n",
    "    print(f\"Best BLEU epoch: {best_bleu_epoch + 1}\")\n",
    "    print(f\"Best BLEU score: {trainer.history['val_bleu'][best_bleu_epoch]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generate Sample Captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model\n",
    "best_model_path = os.path.join(config['save_dir'], 'best_model_loss.pth')\n",
    "trainer.load_checkpoint(best_model_path)\n",
    "model = trainer.model.to(config['device'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate captions for some validation images\n",
    "def generate_caption(image, model, vocab, device):\n",
    "    \"\"\"Generate a caption for an image.\"\"\"\n",
    "    \n",
    "    caption_text = ...\n",
    "    # TODO: Implement the caption generation function\n",
    "    # 1. Set the model to evaluation mode\n",
    "    # 2. Use torch.no_grad() to disable gradient calculation during inference\n",
    "    # 3. Move the image to the device and add a batch dimension if needed\n",
    "    # 4. Generate a caption using the model's generate_caption method\n",
    "    # 5. Decode the caption indices to text using the vocabulary\n",
    "    # 6. Return the caption text\n",
    "    \n",
    "    return caption_text\n",
    "\n",
    "# Get some validation examples\n",
    "num_examples = 5\n",
    "val_examples = []\n",
    "\n",
    "for images, captions, image_ids in val_loader:\n",
    "    if len(val_examples) >= num_examples:\n",
    "        break\n",
    "    \n",
    "    # Generate captions\n",
    "    for i in range(len(images)):\n",
    "        if len(val_examples) >= num_examples:\n",
    "            break\n",
    "        \n",
    "        image = images[i]\n",
    "        true_caption = vocab.decode(captions[i], join=True, remove_special=True)\n",
    "        generated_caption = generate_caption(image, model, vocab, config['device'])\n",
    "        \n",
    "        val_examples.append({\n",
    "            'image': image,\n",
    "            'image_id': image_ids[i],\n",
    "            'true_caption': true_caption,\n",
    "            'generated_caption': generated_caption\n",
    "        })\n",
    "\n",
    "# Display examples\n",
    "plt.figure(figsize=(15, 5 * num_examples))\n",
    "\n",
    "for i, example in enumerate(val_examples):\n",
    "    plt.subplot(num_examples, 1, i + 1)\n",
    "    \n",
    "    # Convert tensor to image\n",
    "    img = example['image'].permute(1, 2, 0).numpy()\n",
    "    img = (img * [0.229, 0.224, 0.225]) + [0.485, 0.456, 0.406]  # Denormalize\n",
    "    img = np.clip(img, 0, 1)\n",
    "    \n",
    "    plt.imshow(img)\n",
    "    plt.title(f\"Image: {example['image_id']}\")\n",
    "    plt.axis('off')\n",
    "    plt.figtext(0.5, 0.01 + i * (1/num_examples), f\"True caption: {example['true_caption']}\", \n",
    "                ha='center', fontsize=12, bbox={\"facecolor\":\"white\", \"alpha\":0.5, \"pad\":5})\n",
    "    plt.figtext(0.5, 0.05 + i * (1/num_examples), f\"Generated caption: {example['generated_caption']}\", \n",
    "                ha='center', fontsize=12, bbox={\"facecolor\":\"lightgreen\", \"alpha\":0.5, \"pad\":5})\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Compare Greedy and Beam Search Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare greedy decoding vs. beam search\n",
    "def compare_decoding_methods(image, model, vocab, device, beam_sizes=[1, 3, 5]):\n",
    "    \"\"\"Compare different beam search sizes for caption generation.\"\"\"\n",
    "    results = ...\n",
    "    # TODO: Implement a function to compare different beam search settings\n",
    "    # 1. Set the model to evaluation mode\n",
    "    # 2. Initialize a dictionary to store results\n",
    "    # 3. For each beam size:\n",
    "    #    a. Generate a caption using that beam size\n",
    "    #    b. Decode the caption indices to text\n",
    "    #    c. Store the result in the dictionary\n",
    "    # 4. Return the dictionary of results\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Select a random example from val_examples\n",
    "import random\n",
    "example = random.choice(val_examples)\n",
    "\n",
    "# Compare decoding methods\n",
    "beam_results = compare_decoding_methods(\n",
    "    example['image'], \n",
    "    model, \n",
    "    vocab, \n",
    "    config['device'], \n",
    "    beam_sizes=[1, 3, 5]\n",
    ")\n",
    "\n",
    "# Display the image with different captions\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "# Display image\n",
    "img = example['image'].permute(1, 2, 0).numpy()\n",
    "img = (img * [0.229, 0.224, 0.225]) + [0.485, 0.456, 0.406]  # Denormalize\n",
    "img = np.clip(img, 0, 1)\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.title(f\"Image: {example['image_id']}\")\n",
    "\n",
    "# Add captions\n",
    "captions = [\n",
    "    f\"True caption: {example['true_caption']}\",\n",
    "    f\"Greedy search: {beam_results['beam_1']}\",\n",
    "    f\"Beam search (k=3): {beam_results['beam_3']}\",\n",
    "    f\"Beam search (k=5): {beam_results['beam_5']}\"\n",
    "]\n",
    "\n",
    "# Display captions below the image\n",
    "plt.figtext(0.5, 0.01, '\\n'.join(captions), ha='center', fontsize=12, \n",
    "            bbox={\"facecolor\":\"white\", \"alpha\":0.8, \"pad\":5})\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.2, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Final Model and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save configuration\n",
    "import json\n",
    "config_path = os.path.join(config['save_dir'], 'config.json')\n",
    "\n",
    "# Convert non-serializable values to strings\n",
    "serializable_config = {k: str(v) if not isinstance(v, (int, float, str, bool)) else v \n",
    "                      for k, v in config.items()}\n",
    "\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(serializable_config, f, indent=4)\n",
    "\n",
    "print(f\"Configuration saved to {config_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save vocabulary separately for easy access\n",
    "import pickle\n",
    "vocab_path = os.path.join(config['save_dir'], 'vocabulary.pkl')\n",
    "with open(vocab_path, 'wb') as f:\n",
    "    pickle.dump(vocab, f)\n",
    "\n",
    "print(f\"Vocabulary saved to {vocab_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we have successfully:\n",
    "\n",
    "1. Set up the data loaders for training and validation\n",
    "2. Built the caption model by combining the encoder and decoder\n",
    "3. Trained the model with teacher forcing\n",
    "4. Monitored the training progress and validation performance\n",
    "5. Generated captions for sample images\n",
    "6. Compared different decoding strategies (greedy vs. beam search)\n",
    "7. Saved the model, configuration, and vocabulary for later use\n",
    "\n",
    "In the next notebook, we will perform a comprehensive evaluation of the model on the test set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "image_captioning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
